{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H_Gm-cSqpXa1"
   },
   "source": [
    "# Analyse de sentiments\n",
    "\n",
    "C'est désormais un problème classique de machine learning. D'un côté, du texte, de l'autre une appréciation, le plus souvent binaire, positive ou négative mais qui pourrait être graduelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_paTGYppXa7"
   },
   "source": [
    "## Les données\n",
    "\n",
    "On récupère les données depuis le site UCI [Sentiment Labelled Sentences Data Set](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences). \n",
    "\n",
    "Il y a trois fichiers textes à importer et à concaténer pour former un dataset avec deux colonnes:\n",
    "- Une \"texte\" comportant le contenu du commentaire\n",
    "- Une \"sentiment\" comportant une note de 0 ou 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oXDvLY2ZDVkO"
   },
   "source": [
    "#### Questions préalables: \n",
    "- Quelle est la taille du dataset ?\n",
    "- Comment est distribuée la variable sentiment ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Bri_R2PDfGv"
   },
   "source": [
    "#### Question 1: Combien y a t'il de mots dans l'ensemble du corpus ? Sont-ils \"homogènes\" ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EscnTgSGdnsV"
   },
   "source": [
    "### Import du module permettant le traitement de texte: nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NmkgemE8gLIz"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from  module_nlp import *\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.naive_bayes import CategoricalNB,BernoulliNB,ComplementNB,GaussianNB,MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4VzY9vw9DyNq"
   },
   "source": [
    "#### Consigne 1: Récupérer le corpus de mots sans la ponctuation ni les stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'sentiment labelled sentences/'\n",
    "\n",
    "txts=[]\n",
    "# iterate over files in\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNtZjfkm3KuJ"
   },
   "outputs": [],
   "source": [
    "# assign directory\n",
    "directory = 'sentiment labelled sentences/'\n",
    "dic={}\n",
    "txts=[]\n",
    "# iterate over files in\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "\n",
    "    if \"labelled\" in filename:\n",
    "        name=filename.split(\"_\")[0]\n",
    "\n",
    "        if os.path.isfile(f):\n",
    "            if f.endswith('.txt'):\n",
    "                with open(f,'r', encoding=\"utf-8\") as file:\n",
    "                    txt = file.read()\n",
    "                txts.append(txt)\n",
    "                dic[name]={'txt':txt}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58226"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic['amazon']['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['amazon', 'imdb', 'yelp'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['mix']={\n",
    "    'text':[],\n",
    "    'site':[],\n",
    "    'target':[],\n",
    "    'clean':[],\n",
    "    'clean1':[],\n",
    "    'clean2':[]\n",
    "}\n",
    "\n",
    "\n",
    "for k,v in dic.items():\n",
    "    if 'txt' in dic[k].keys():\n",
    "        txt=dic[k]['txt']\n",
    "        lst = txt.split(\"\\n\")\n",
    "\n",
    "        dic[k]['tup']=[]\n",
    "        dic[k]['clean_tup']=[]\n",
    "        dic[k]['vec_tup']=[]\n",
    "        dic[k]['target']=[]\n",
    "        t_lst=[]\n",
    "\n",
    "        for line in lst:\n",
    "            if len(line.split(\"\\t\"))==2:\n",
    "                t,n = tuple(line.split(\"\\t\"))\n",
    "\n",
    "                dic[k]['target'].append(n)\n",
    "                \n",
    "\n",
    "                dic[k]['tup'].append((t,n))\n",
    "\n",
    "                c1 =(clean(t))\n",
    "                c2=(clean(t,2))\n",
    "                dic[k]['clean_tup'].append((c1,n))\n",
    "\n",
    "                dic['mix']['target'].append(n)\n",
    "                dic['mix']['site'].append(k)\n",
    "                dic['mix']['text'].append(t)\n",
    "\n",
    "                dic['mix']['clean1'].append(c1)\n",
    "                dic['mix']['clean2'].append(c2)\n",
    "                \n",
    "\n",
    "                t_lst.append(t)\n",
    "\n",
    "            # dic[k]['clean_tup'].append((t,n))\n",
    "\n",
    "    # dic[k]['tfidf'] = tfidf(t_lst)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a=tfidf(dic['mix']['text'])[1]\n",
    "df_b=pd.DataFrame({\n",
    "    'site1':dic['mix']['site'],\n",
    "    'target':dic['mix']['target']\n",
    "    })\n",
    "\n",
    "df = pd.concat([df_a, df_b],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>15g</th>\n",
       "      <th>15pm</th>\n",
       "      <th>17</th>\n",
       "      <th>...</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yun</th>\n",
       "      <th>z500a</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombiez</th>\n",
       "      <th>site1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>amazon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>amazon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>amazon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yelp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yelp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yelp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yelp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yelp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 5024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00   10  100   11   12   13   15  15g  15pm   17  ...  yum  yummy  yun  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  ...  0.0    0.0  0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  ...  0.0    0.0  0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  ...  0.0    0.0  0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  ...  0.0    0.0  0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  ...  0.0    0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...   ...  ...  ...  ...    ...  ...   \n",
       "2995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  ...  0.0    0.0  0.0   \n",
       "2996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  ...  0.0    0.0  0.0   \n",
       "2997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  ...  0.0    0.0  0.0   \n",
       "2998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  ...  0.0    0.0  0.0   \n",
       "2999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  ...  0.0    0.0  0.0   \n",
       "\n",
       "      z500a  zero  zillion  zombie  zombiez   site1  target  \n",
       "0       0.0   0.0      0.0     0.0      0.0  amazon       0  \n",
       "1       0.0   0.0      0.0     0.0      0.0  amazon       1  \n",
       "2       0.0   0.0      0.0     0.0      0.0  amazon       1  \n",
       "3       0.0   0.0      0.0     0.0      0.0  amazon       0  \n",
       "4       0.0   0.0      0.0     0.0      0.0  amazon       1  \n",
       "...     ...   ...      ...     ...      ...     ...     ...  \n",
       "2995    0.0   0.0      0.0     0.0      0.0    yelp       0  \n",
       "2996    0.0   0.0      0.0     0.0      0.0    yelp       0  \n",
       "2997    0.0   0.0      0.0     0.0      0.0    yelp       0  \n",
       "2998    0.0   0.0      0.0     0.0      0.0    yelp       0  \n",
       "2999    0.0   0.0      0.0     0.0      0.0    yelp       0  \n",
       "\n",
       "[3000 rows x 5024 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['amazon', 'imdb', 'yelp'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['site1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.802\n",
      "Test Score :  0.825\n",
      "----\n",
      "Mean: 0.802\n",
      "Test Score :  0.798\n",
      "----\n",
      "Mean: 0.804\n",
      "Test Score :  0.812\n",
      "----\n",
      "Mean: 0.798\n",
      "Test Score :  0.805\n",
      "----\n",
      "Mean: 0.798\n",
      "Test Score :  0.807\n",
      "----\n",
      "Mean: 0.799\n",
      "Test Score :  0.798\n",
      "----\n",
      "Mean: 0.8\n",
      "Test Score :  0.807\n",
      "----\n",
      "Mean: 0.799\n",
      "Test Score :  0.797\n",
      "----\n",
      "[[0.802, 0.825], [0.802, 0.798], [0.804, 0.812], [0.798, 0.805], [0.798, 0.807], [0.799, 0.798], [0.8, 0.807], [0.799, 0.797]]\n"
     ]
    }
   ],
   "source": [
    "c_lst=['text','clean1']\n",
    "\n",
    "mods=[\n",
    "    #KNeighborsClassifier(),\n",
    "    LogisticRegression(),\n",
    "    #CategoricalNB(class_prior=df.shape),\n",
    "    BernoulliNB(),\n",
    "    ComplementNB(),\n",
    "    #GaussianNB(),\n",
    "    MultinomialNB()\n",
    "    ]\n",
    "l=[]\n",
    "for v in c_lst:\n",
    "    l.extend(v*len(mods))\n",
    "\n",
    "arrays = [\n",
    "        mods * len(c_lst),\n",
    "        c_lst * len(mods)\n",
    "    ]\n",
    "\n",
    "tuples = list(zip(*arrays))\n",
    "    \n",
    "rm=[] \n",
    "for mod in mods:\n",
    "    rc=[]\n",
    "    for c in c_lst:\n",
    "        # print(mod, c)\n",
    "        \n",
    "        df_a=tfidf(dic['mix'][c])[1]\n",
    "        df_b=pd.DataFrame({\n",
    "            'site1':dic['mix']['site'],\n",
    "            'target':dic['mix']['target']\n",
    "            })\n",
    "\n",
    "        df = pd.concat([df_a, df_b],axis=1)\n",
    "        df = ohe(df,['site1'])\n",
    "        \n",
    "        X=df.drop('target',axis=1)\n",
    "        y=df.target\n",
    "        X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,train_size=0.8)\n",
    "\n",
    "        \n",
    "        clf = mod\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=10, scoring = \"accuracy\")\n",
    "        # print(\"Scores:\", scores)\n",
    "        print(\"Mean:\", round(scores.mean(),3))\n",
    "        # print(\"Standard Deviation:\", round(scores.std()*100, 2))\n",
    "        clf.fit(X_train,y_train)\n",
    "        print(\"Test Score : \",round(clf.score(X_test,y_test),3))\n",
    "        print(\"----\")\n",
    "\n",
    "        rm.append([round(scores.mean(),3),round(clf.score(X_test,y_test),3)])\n",
    "\n",
    "    # rm.append(rc)\n",
    "\n",
    "print(rm)\n",
    "\n",
    "# res= pd.Series(rm,index = arrays)\n",
    "# res\n",
    "\n",
    "# results2 = pd.DataFrame({\n",
    "# 'Model': ['KNN', 'Logistic Regression', 'Random Forest', 'MultinomialNB', 'GaussianNB'],\n",
    "# 'Score': [ acc_knn, acc_logit, acc_rf, acc_mnb, acc_gnb]\n",
    "# })\n",
    "    \n",
    "    # result_df2 = results2.sort_values(by='Score', ascending=False)\n",
    "    # result_df2 = result_df2.set_index('Score')\n",
    "    # result_df2\n",
    "    # print('____')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all arrays must be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27096\\2414877797.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cv mean'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# .sort_values('test',ascending=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# results2 = pd.DataFrame({\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    700\u001b[0m                         \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                     )\n\u001b[1;32m--> 702\u001b[1;33m                     mgr = arrays_to_mgr(\n\u001b[0m\u001b[0;32m    703\u001b[0m                         \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                         \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mensure_index\u001b[1;34m(index_like, copy)\u001b[0m\n\u001b[0;32m   6329\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6331\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6332\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6333\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\multi.py\u001b[0m in \u001b[0;36mfrom_arrays\u001b[1;34m(cls, arrays, sortorder, names)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"all arrays must be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactorize_from_iterables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all arrays must be same length"
     ]
    }
   ],
   "source": [
    "res= pd.DataFrame(rm,index = arrays,columns=['cv mean','test'])\n",
    "res\n",
    "# .sort_values('test',ascending=False)\n",
    "\n",
    "# results2 = pd.DataFrame({\n",
    "# 'Model': ['KNN', 'Logistic Regression', 'Random Forest', 'MultinomialNB', 'GaussianNB'],\n",
    "# 'Score': [ acc_knn, acc_logit, acc_rf, acc_mnb, acc_gnb]\n",
    "# })\n",
    "    \n",
    "#     result_df2 = results2.sort_values(by='Score', ascending=False)\n",
    "#     result_df2 = result_df2.set_index('Score')\n",
    "#     result_df2\n",
    "#     print('____')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e_goy\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "50 fits failed out of a total of 75.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\e_goy\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\e_goy\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\e_goy\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\e_goy\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\e_goy\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\e_goy\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\e_goy\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.74625           nan        nan 0.80041667        nan\n",
      "        nan 0.79916667        nan        nan 0.7725            nan\n",
      "        nan 0.75875           nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.1, 1, 10, 100, 1000], 'max_iter': [1000],\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet']})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,train_size=0.8)\n",
    "\n",
    "params={\n",
    "    'penalty':['l1','l2','elasticnet'],\n",
    "    'C':[0.1,1,10,100,1000],\n",
    "    #'solver':['liblinear','newton-cg']\n",
    "    'max_iter':[1000]\n",
    "}\n",
    "\n",
    "clf=GridSearchCV(LogisticRegression(),params,cv=5)\n",
    "clf.fit(X_train,y_train)\n",
    "# print(\"Test Score : \",round(clf.score(X_test,y_test),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,train_size=0.8)\n",
    "\n",
    "params={\n",
    "    'penalty':['l1','l2','elasticnet'],\n",
    "    'C':[0.1,1,10,100,1000],\n",
    "    #'solver':['liblinear','newton-cg']\n",
    "    'max_iter':[1000]\n",
    "}\n",
    "\n",
    "clf=GridSearchCV(LogisticRegression(),params,cv=5)\n",
    "clf.fit(X_train,y_train)\n",
    "# print(\"Test Score : \",round(clf.score(X_test,y_test),3))\n",
    "# BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon\n",
      "imdb\n",
      "yelp\n"
     ]
    }
   ],
   "source": [
    "for i in dic:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon\n",
      "Counter({'0': 500, '1': 500})\n",
      "imdb\n",
      "Counter({'0': 500, '1': 500})\n",
      "yelp\n",
      "Counter({'1': 500, '0': 500})\n"
     ]
    }
   ],
   "source": [
    "for k,v in dic.items():\n",
    "    print(k)\n",
    "    print(Counter(dic[k]['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \\t0\\nNot sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  \\t0\\nAttempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.  \\t0\\nVery little music or anything to speak of.  \\t0\\nThe best scene in the movie was when Gerardo is trying to find a song that keeps running through his head.  \\t1\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=txts[1][:541]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \\t0'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=test.split(\"\\n\")[0]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  ',\n",
       " '0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple( t.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d_--nXldFphn"
   },
   "source": [
    "#### Question 2: Combien de mots restent-ils ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xcQXTEey3LaZ"
   },
   "outputs": [],
   "source": [
    "clean_txts={\n",
    "    'tokens':[],\n",
    "    'txt':[]\n",
    "}\n",
    "mix={\n",
    "    'tokens':[],\n",
    "    'txt':\"\"\n",
    "}\n",
    "\n",
    "for txt in txts:\n",
    "    t=(clean(txt))\n",
    "    clean_txts['tokens'].append(t)\n",
    "    clean_txts['txt'].append(\" \".join(t))\n",
    "\n",
    "    mix['tokens'] += t\n",
    "    mix['txt'] += \" \".join(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb token :  3422\n",
      "nb token unique:  1103\n",
      "---\n",
      "nb token :  5322\n",
      "nb token unique:  2116\n",
      "---\n",
      "nb token :  76\n",
      "nb token unique:  48\n",
      "---\n",
      "nb token :  3950\n",
      "nb token unique:  1302\n",
      "---\n",
      "all nb token :  12770\n",
      "all nb token unique:  3588\n"
     ]
    }
   ],
   "source": [
    "for i in clean_txts['tokens']:\n",
    "    print (\"nb token : \",len(i))\n",
    "    print (\"nb token unique: \",len(set(i)))\n",
    "    print(\"---\")\n",
    "\n",
    "print (\"all nb token : \",len(mix['tokens']))\n",
    "print (\"all nb token unique: \",len(set(mix['tokens'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZSwQ69FpXa-"
   },
   "source": [
    "## Exercice 1 : approche td-idf\n",
    "\n",
    "La cible est la colonne *sentiment*, les deux autres colonnes sont les features. Il faudra utiliser les prétraitements [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html), [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), [TF-IDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). L'un d'entre eux n'est pas nécessaire depuis la version [0.20.0](http://scikit-learn.org/stable/whats_new.html#sklearn-preprocessing) de *scikit-learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x2_dDBUxEoc2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4s9adDJEpsx"
   },
   "source": [
    "#### Consigne 2: Utiliser OneHotEncoder ou CountVectorizer pour représenter les données sous forme d'une matrice contenant autant de colonnes que de mots dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qunw7as0pXa_"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUcM-S7SFb-5"
   },
   "source": [
    "#### Consigne 3: lancer un modèle de classification pour prédire les sentiments de 30% des observations\n",
    "\n",
    "\n",
    "\n",
    "*   Séparer échantillon train / test\n",
    "*   Entrainer un modèle de classification\n",
    "*   Afficher la matrice de confusion\n",
    "*   Calculer l'accuracy, la précision et le recall\n",
    "*   Votre modèle est-il soumis à un overfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWXIE3jGK5sS"
   },
   "source": [
    "#### Consigne 4: Utiliser TF-IDF pour représenter les données et relancer un modèle de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O-l47TebpXbA"
   },
   "source": [
    "## Exercice 2 : word2vec\n",
    "\n",
    "On utilise l'approche [word2vec](https://en.wikipedia.org/wiki/Word2vec) du module [gensim](https://radimrehurek.com/gensim/models/word2vec.html) ou [spacy](https://spacy.io/usage/vectors-similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Z0QugRgkXDE"
   },
   "source": [
    "#### Consigne 5: Utiliser Word2vec pour représenter les données et relancer un modèle de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lke42PWFpXbB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8RiKV-gSpXbC"
   },
   "source": [
    "## Exercice 3 : Comparer les deux approches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ia88RvTrkemu"
   },
   "source": [
    "#### Consigne 6: Faire un graphique montrant les courbes ROC associées au deux modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PjiCxvFJg4TQ"
   },
   "source": [
    "#### Consigne 7: Faire un graphique montrant les courbes Precision-Recall associées aux deux modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hfAqKn0ehUHl"
   },
   "source": [
    "### BONUS: reprendre l'étape de One_hot_encoding et ajouter une étape de réduction de dimension (type ACP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gu5Lbz2rg22u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQS0UJb1DaAi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Q_8h1ArpXbD"
   },
   "outputs": [],
   "source": [
    "knight base"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TD2A_sentiment_analysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
